{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Mid-Level API: Composable Building Blocks\n",
    "\n",
    "**Goal:** Build custom transfer workflows using composable components.\n",
    "\n",
    "This is for users who need more control but don't want to handle low-level details.\n",
    "\n",
    "## Why This Matters\n",
    "\n",
    "- **Flexible**: Mix and match components for custom workflows\n",
    "- **Reusable**: Build patterns once, use them everywhere  \n",
    "- **Transparent**: See exactly what each step does\n",
    "- **Composable**: Chain operations like LEGO blocks\n",
    "\n",
    "## Real World Usage\n",
    "\n",
    "Data engineers who need custom pipelines but want to stay productive. Build once, reuse everywhere.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snowpark_db_api import TransferBuilder, ConnectionManager\n",
    "from snowpark_db_api.transforms import Pipeline, SchemaTransform, QueryTransform, show_pipeline_steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 1. TransferBuilder Pattern\n",
    "\n",
    "Build transfers step by step using fluent interface. Each step is explicit and reusable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline Analysis\n",
      "==================================================\n",
      "Pipeline has 2 transforms:\n",
      "  1. SchemaTransform\n",
      "  2. QueryTransform\n",
      "\n",
      "Sample transformation with input: (SELECT TOP 1000 * FROM dbo.Orders WHERE orderdate >= '2023-01-01') AS recent_orders\n",
      "  Step 1 (SchemaTransform): Schema mapping configured (3 columns)\n",
      "  Step 2 (QueryTransform): Query processed → BUILDER_TEST_TABLE\n",
      "\n",
      "Pipeline Analysis Complete\n",
      "   All transforms are properly configured and ready for execution\n",
      "🏗️ Executing custom pipeline: (SELECT TOP 1000 * FROM dbo.Orders WHERE orderdate >= '2023-01-01') AS recent_orders → BUILDER_TEST_TABLE\n",
      "📋 Pipeline steps: ['SchemaTransform', 'QueryTransform']\n",
      "2025-07-03 16:11:13 - snowpark_db_api.core - INFO - Setting up connections\n",
      "2025-07-03 16:11:14 - snowpark_db_api.core - INFO - All connections established\n",
      "2025-07-03 16:11:14 - snowpark_db_api.core - INFO - Starting transfer using custom query -> BUILDER_TEST_TABLE\n",
      "2025-07-03 16:11:15 - snowpark_db_api.connections - INFO - Successfully connected to SQL Server: alingtestserver.database.windows.net\n",
      "2025-07-03 16:11:16 - snowpark_db_api.connections - INFO - Successfully connected to SQL Server: alingtestserver.database.windows.net\n",
      "2025-07-03 16:11:17 - snowpark_db_api.connections - INFO - Successfully connected to SQL Server: alingtestserver.database.windows.net\n",
      "2025-07-03 16:11:17 - snowpark_db_api.core - ERROR - Query transfer failed: ('42S22', \"[42S22] [Microsoft][ODBC Driver 18 for SQL Server][SQL Server]Invalid column name 'orderdate'. (207) (SQLExecDirectW)\")\n",
      "2025-07-03 16:11:17 - snowpark_db_api.core - ERROR - Transfer failed: Both approaches failed: Failed to infer Snowpark DataFrame schema from '(SELECT TOP 1000 * FROM dbo.Orders WHERE orderdate >= '2023-01-01') AS recent_orders' due to ProgrammingError('42S22', \"[42S22] [Microsoft][ODBC Driver 18 for SQL Server][SQL Server]Invalid column name 'orderdate'. (207) (SQLExecDirectW)\"). To avoid auto inference, you can manually specify the Snowpark DataFrame schema using 'custom_schema' in DataFrameReader.dbapi. Please check the stack trace for more details., ('42S22', \"[42S22] [Microsoft][ODBC Driver 18 for SQL Server][SQL Server]Invalid column name 'orderdate'. (207) (SQLExecDirectW)\")\n",
      "Basic transfer result: False\n"
     ]
    }
   ],
   "source": [
    "# Build a custom transfer workflow\n",
    "result = (TransferBuilder()\n",
    "         .from_source(\"(SELECT TOP 50 ID, Column0, Column1 FROM dbo.RandomDataWith100Columns) AS builder_test\")\n",
    "         .to_destination(\"BUILDER_TEST_TABLE\") \n",
    "         .with_schema_mapping()  # Custom schema mapping\n",
    "         .with_query_optimization()  # Query optimization\n",
    "         .show_pipeline_steps(True)  # Show what's happening\n",
    "         .execute())\n",
    "\n",
    "print(f\"Basic transfer result: {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline Analysis\n",
      "==================================================\n",
      "Pipeline has 3 transforms:\n",
      "  1. SchemaTransform\n",
      "  2. QueryTransform\n",
      "  3. ConnectionTransform\n",
      "\n",
      "Sample transformation with input: (SELECT TOP 1000 * FROM dbo.RandomDataWith100Columns WHERE Column1 IS NOT NULL) AS sample_data\n",
      "  Step 1 (SchemaTransform): Schema mapping configured (3 columns)\n",
      "  Step 2 (QueryTransform): Query processed → SAMPLE_DATA\n",
      "  Step 3 (ConnectionTransform): Connection optimized → {'fetch_size': 1000, 'max_workers': 4, 'timeout': 300}\n",
      "\n",
      "Pipeline Analysis Complete\n",
      "   All transforms are properly configured and ready for execution\n",
      "🏗️ Executing custom pipeline: (SELECT TOP 1000 * FROM dbo.RandomDataWith100Columns WHERE Column1 IS NOT NULL) AS sample_data → SAMPLE_DATA\n",
      "📋 Pipeline steps: ['SchemaTransform', 'QueryTransform', 'ConnectionTransform']\n",
      "2025-07-03 16:02:46 - snowpark_db_api.core - INFO - Setting up connections\n",
      "2025-07-03 16:02:48 - snowpark_db_api.core - INFO - All connections established\n",
      "2025-07-03 16:02:48 - snowpark_db_api.core - INFO - Starting transfer using custom query -> SAMPLE_DATA\n",
      "2025-07-03 16:02:49 - snowpark_db_api.connections - INFO - Successfully connected to SQL Server: alingtestserver.database.windows.net\n",
      "2025-07-03 16:02:49 - snowpark_db_api.connections - INFO - Successfully connected to SQL Server: alingtestserver.database.windows.net\n",
      "2025-07-03 16:02:51 - snowpark_db_api.connections - INFO - Successfully connected to SQL Server: alingtestserver.database.windows.net\n",
      "2025-07-03 16:02:55 - snowpark_db_api.core - INFO - Transferring 1,000 rows\n",
      "2025-07-03 16:02:56 - snowpark_db_api.core - INFO - Transfer completed: 1,000 rows in 8.7s\n",
      "Custom pipeline result: True\n"
     ]
    }
   ],
   "source": [
    "# Custom query with pipeline optimization\n",
    "result = (TransferBuilder()\n",
    "    .from_source(\"(SELECT TOP 1000 * FROM dbo.RandomDataWith100Columns WHERE Column1 IS NOT NULL) AS sample_data\")\n",
    "    .to_destination(\"SAMPLE_DATA\")\n",
    "    .with_schema_mapping({'VARCHAR': 'STRING', 'DATETIME': 'TIMESTAMP'})\n",
    "    .with_query_optimization()\n",
    "    .with_environment('production')\n",
    "    .show_pipeline_steps(True)\n",
    "    .execute())\n",
    "\n",
    "print(f\"Custom pipeline result: {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline Analysis\n",
      "==================================================\n",
      "Pipeline has 3 transforms:\n",
      "  1. SchemaTransform\n",
      "  2. QueryTransform\n",
      "  3. ConnectionTransform\n",
      "\n",
      "Sample transformation with input: (SELECT TOP 15 Id, FullName FROM dbo.UserProfile WHERE Id IS NOT NULL) AS advanced_test\n",
      "  Step 1 (SchemaTransform): Schema mapping configured (3 columns)\n",
      "  Step 2 (QueryTransform): Query processed → ADVANCED_WORKFLOW_TABLE\n",
      "  Step 3 (ConnectionTransform): Connection optimized → {'fetch_size': 1000, 'max_workers': 4, 'timeout': 300}\n",
      "\n",
      "Pipeline Analysis Complete\n",
      "   All transforms are properly configured and ready for execution\n",
      "🏗️ Executing custom pipeline: (SELECT TOP 15 Id, FullName FROM dbo.UserProfile WHERE Id IS NOT NULL) AS advanced_test → ADVANCED_WORKFLOW_TABLE\n",
      "📋 Pipeline steps: ['SchemaTransform', 'QueryTransform', 'ConnectionTransform']\n",
      "2025-07-03 16:03:03 - snowpark_db_api.core - INFO - Setting up connections\n",
      "2025-07-03 16:03:04 - snowpark_db_api.core - INFO - All connections established\n",
      "2025-07-03 16:03:04 - snowpark_db_api.core - INFO - Starting transfer using custom query -> ADVANCED_WORKFLOW_TABLE\n",
      "2025-07-03 16:03:05 - snowpark_db_api.connections - INFO - Successfully connected to SQL Server: alingtestserver.database.windows.net\n",
      "2025-07-03 16:03:06 - snowpark_db_api.connections - INFO - Successfully connected to SQL Server: alingtestserver.database.windows.net\n",
      "2025-07-03 16:03:07 - snowpark_db_api.connections - INFO - Successfully connected to SQL Server: alingtestserver.database.windows.net\n",
      "2025-07-03 16:03:10 - snowpark_db_api.core - INFO - Transferring 1 rows\n",
      "2025-07-03 16:03:11 - snowpark_db_api.core - INFO - Transfer completed: 1 rows in 6.9s\n"
     ]
    }
   ],
   "source": [
    "# Complex workflow with multiple configurations\n",
    "result = (TransferBuilder()\n",
    "         .from_source(\"(SELECT TOP 15 Id, FullName FROM dbo.UserProfile WHERE Id IS NOT NULL) AS advanced_test\")\n",
    "         .to_destination(\"ADVANCED_WORKFLOW_TABLE\")\n",
    "         .with_schema_mapping({'Id': 'USER_ID', 'FullName': 'FULL_NAME'})  # Custom mappings\n",
    "         .with_query_optimization({'complexity': 'simple'})  # Optimization hints\n",
    "         .with_environment('production')  # Production settings\n",
    "         .show_pipeline_steps(True)  # Show transparency\n",
    "         .execute())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 2. Connection Management\n",
    "\n",
    "Explicit control over database connections. Perfect for batch processing or long-running operations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Establishing connections...\n",
      "🔌 Establishing database connections\n",
      "2025-07-03 01:50:50 - snowpark_db_api.core - INFO - Setting up connections\n",
      "2025-07-03 01:50:51 - snowpark_db_api.core - INFO - All connections established\n",
      "Connection established: True\n",
      "Step 2: Testing connections...\n",
      "2025-07-03 01:50:52 - snowpark_db_api.connections - INFO - Successfully connected to SQL Server: alingtestserver.database.windows.net\n",
      "✅ Source database connection OK\n",
      "✅ Snowflake connection OK\n",
      "Test results: {'source_db': True, 'snowflake': True}\n",
      "Step 3: Executing multiple transfers...\n",
      "2025-07-03 01:50:52 - snowpark_db_api.core - INFO - Starting transfer using custom query -> RANDOMDATAWITH100COLUMNS\n",
      "2025-07-03 01:50:53 - snowpark_db_api.connections - INFO - Successfully connected to SQL Server: alingtestserver.database.windows.net\n",
      "2025-07-03 01:50:54 - snowpark_db_api.connections - INFO - Successfully connected to SQL Server: alingtestserver.database.windows.net\n",
      "2025-07-03 01:50:55 - snowpark_db_api.connections - INFO - Successfully connected to SQL Server: alingtestserver.database.windows.net\n",
      "2025-07-03 01:50:57 - snowpark_db_api.core - INFO - Transferring 5 rows\n",
      "2025-07-03 01:50:58 - snowpark_db_api.core - INFO - Transfer completed: 5 rows in 6.5s\n",
      "Transfer result: True\n",
      "2025-07-03 01:50:58 - snowpark_db_api.core - INFO - Starting transfer using custom query -> RANDOMDATAWITH100COLUMNS\n",
      "2025-07-03 01:50:59 - snowpark_db_api.connections - INFO - Successfully connected to SQL Server: alingtestserver.database.windows.net\n",
      "2025-07-03 01:51:00 - snowpark_db_api.connections - INFO - Successfully connected to SQL Server: alingtestserver.database.windows.net\n",
      "2025-07-03 01:51:02 - snowpark_db_api.connections - INFO - Successfully connected to SQL Server: alingtestserver.database.windows.net\n",
      "2025-07-03 01:51:04 - snowpark_db_api.core - INFO - Transferring 1 rows\n",
      "2025-07-03 01:51:05 - snowpark_db_api.core - INFO - Transfer completed: 1 rows in 6.7s\n",
      "Transfer result: True\n",
      "Step 4: Cleaning up connections...\n",
      "🔌 Connections closed\n",
      "Lifecycle test overall success: True\n"
     ]
    }
   ],
   "source": [
    "# Test complete lifecycle\n",
    "manager = ConnectionManager()\n",
    "\n",
    "# Step 1: Connect\n",
    "print(\"Step 1: Establishing connections...\")\n",
    "connect_success = manager.connect()\n",
    "print(f\"Connection established: {connect_success}\")\n",
    "\n",
    "if not connect_success:\n",
    "    print(\"❌ Failed to establish connections\")\n",
    "\n",
    "# Step 2: Test\n",
    "print(\"Step 2: Testing connections...\")\n",
    "test_results = manager.test_connections()\n",
    "print(f\"Test results: {test_results}\")\n",
    "\n",
    "# Step 3: Execute multiple transfers\n",
    "print(\"Step 3: Executing multiple transfers...\")\n",
    "transfers = [\n",
    "    \"(SELECT TOP 5 ID, Column0 FROM dbo.RandomDataWith100Columns) AS lifecycle_test1\",\n",
    "    \"(SELECT TOP 5 Id, FullName FROM dbo.UserProfile WHERE Id IS NOT NULL) AS lifecycle_test2\"\n",
    "]\n",
    "\n",
    "results = []\n",
    "for transfer_query in transfers:\n",
    "    result = manager.execute_transfer(transfer_query)\n",
    "    results.append(result)\n",
    "    print(f\"Transfer result: {result}\")\n",
    "\n",
    "# Step 4: Clean up\n",
    "print(\"Step 4: Cleaning up connections...\")\n",
    "manager.close()\n",
    "\n",
    "overall_success = all(results)\n",
    "print(f\"Lifecycle test overall success: {overall_success}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 4. Reusable Patterns\n",
    "\n",
    "Build patterns once, use them everywhere. This is where the mid-level API really shines.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Pipeline Analysis\n",
      "==================================================\n",
      "📋 Pipeline has 2 transforms:\n",
      "  1. SchemaTransform\n",
      "  2. ConnectionTransform\n",
      "\n",
      "🧪 Sample transformation with input: dbo.Orders\n",
      "  Step 1 (SchemaTransform): ✅ Schema mapping configured (3 columns)\n",
      "  Step 2 (ConnectionTransform): ✅ Connection optimized → {'fetch_size': 1000, 'max_workers': 4, 'timeout': 300}\n",
      "\n",
      "✅ Pipeline Analysis Complete\n",
      "   All transforms are properly configured and ready for execution\n",
      "🏗️ Executing custom pipeline: dbo.Orders → ORDERS_STANDARDIZED\n",
      "📋 Pipeline steps: ['SchemaTransform', 'ConnectionTransform']\n",
      "2025-07-03 01:55:16 - snowpark_db_api.core - INFO - Setting up connections\n",
      "2025-07-03 01:55:17 - snowpark_db_api.core - INFO - All connections established\n",
      "2025-07-03 01:55:17 - snowpark_db_api.core - INFO - Starting transfer: dbo.Orders -> ORDERS_STANDARDIZED\n",
      "2025-07-03 01:55:18 - snowpark_db_api.connections - INFO - Successfully connected to SQL Server: alingtestserver.database.windows.net\n",
      "2025-07-03 01:55:19 - snowpark_db_api.connections - INFO - Successfully connected to SQL Server: alingtestserver.database.windows.net\n",
      "2025-07-03 01:55:20 - snowpark_db_api.connections - INFO - Successfully connected to SQL Server: alingtestserver.database.windows.net\n",
      "2025-07-03 01:55:27 - snowpark_db_api.core - INFO - Transferring 10,004 rows\n",
      "2025-07-03 01:55:28 - snowpark_db_api.core - INFO - Transfer completed: 10,004 rows in 11.2s\n",
      "Standard pattern result: True\n",
      "🔌 Establishing database connections\n",
      "2025-07-03 01:55:29 - snowpark_db_api.core - INFO - Setting up connections\n",
      "2025-07-03 01:55:29 - snowpark_db_api.core - INFO - All connections established\n",
      "2025-07-03 01:55:29 - snowpark_db_api.core - INFO - Starting transfer: dbo.Orders -> RANDOMDATAWITH100COLUMNS\n",
      "2025-07-03 01:55:30 - snowpark_db_api.connections - INFO - Successfully connected to SQL Server: alingtestserver.database.windows.net\n",
      "2025-07-03 01:55:31 - snowpark_db_api.connections - INFO - Successfully connected to SQL Server: alingtestserver.database.windows.net\n",
      "2025-07-03 01:55:33 - snowpark_db_api.connections - INFO - Successfully connected to SQL Server: alingtestserver.database.windows.net\n",
      "2025-07-03 01:55:40 - snowpark_db_api.core - INFO - Transferring 10,004 rows\n",
      "2025-07-03 01:55:41 - snowpark_db_api.core - INFO - Transfer completed: 10,004 rows in 12.0s\n",
      "2025-07-03 01:55:41 - snowpark_db_api.core - INFO - Starting transfer: dbo.UserProfile -> RANDOMDATAWITH100COLUMNS\n",
      "2025-07-03 01:55:42 - snowpark_db_api.connections - INFO - Successfully connected to SQL Server: alingtestserver.database.windows.net\n",
      "2025-07-03 01:55:43 - snowpark_db_api.connections - INFO - Successfully connected to SQL Server: alingtestserver.database.windows.net\n",
      "2025-07-03 01:55:44 - snowpark_db_api.connections - INFO - Successfully connected to SQL Server: alingtestserver.database.windows.net\n",
      "2025-07-03 01:55:48 - snowpark_db_api.core - INFO - Transferring 1 rows\n",
      "2025-07-03 01:55:49 - snowpark_db_api.core - INFO - Transfer completed: 1 rows in 8.0s\n",
      "2025-07-03 01:55:49 - snowpark_db_api.core - INFO - Starting transfer using custom query -> RANDOMDATAWITH100COLUMNS\n",
      "2025-07-03 01:55:50 - snowpark_db_api.connections - INFO - Successfully connected to SQL Server: alingtestserver.database.windows.net\n",
      "2025-07-03 01:55:51 - snowpark_db_api.connections - INFO - Successfully connected to SQL Server: alingtestserver.database.windows.net\n",
      "2025-07-03 01:55:53 - snowpark_db_api.connections - INFO - Successfully connected to SQL Server: alingtestserver.database.windows.net\n",
      "2025-07-03 01:55:56 - snowpark_db_api.core - INFO - Transferring 100 rows\n",
      "2025-07-03 01:55:57 - snowpark_db_api.core - INFO - Transfer completed: 100 rows in 7.7s\n",
      "🔌 Connections closed\n",
      "Batch results: [('dbo.Orders', True), ('dbo.UserProfile', True), ('(SELECT TOP 100 * FROM dbo.RandomDataWith100Columns) AS sample', True)]\n",
      "♻️ Patterns can be reused across projects and teams!\n"
     ]
    }
   ],
   "source": [
    "# Define reusable transfer patterns\n",
    "def create_sql_server_to_snowflake_pattern():\n",
    "    \"\"\"Standard pattern for SQL Server to Snowflake transfers\"\"\"\n",
    "    return (TransferBuilder()\n",
    "            .with_schema_mapping({\n",
    "                'VARCHAR': 'STRING',\n",
    "                'NVARCHAR': 'STRING', \n",
    "                'DATETIME': 'TIMESTAMP',\n",
    "                'DATETIME2': 'TIMESTAMP',\n",
    "                'BIT': 'BOOLEAN'\n",
    "            })\n",
    "            .with_environment('production')\n",
    "            .show_pipeline_steps(True))\n",
    "\n",
    "def create_batch_transfer_pattern(table_list):\n",
    "    \"\"\"Reusable pattern for batch transfers\"\"\"\n",
    "    conn = ConnectionManager()\n",
    "    try:\n",
    "        conn.connect()\n",
    "        results = []\n",
    "        for table in table_list:\n",
    "            result = conn.execute_transfer(table)\n",
    "            results.append((table, result))\n",
    "        return results\n",
    "    finally:\n",
    "        conn.close()  # Ensure cleanup happens\n",
    "\n",
    "# Use the patterns\n",
    "# Pattern 1: Standard transfer with custom schema mapping\n",
    "standard_pattern = create_sql_server_to_snowflake_pattern()\n",
    "result = (standard_pattern\n",
    "    .from_source(\"dbo.Orders\")\n",
    "    .to_destination(\"ORDERS_STANDARDIZED\")\n",
    "    .execute())\n",
    "\n",
    "print(f\"Standard pattern result: {result}\")\n",
    "\n",
    "# Pattern 2: Batch processing\n",
    "tables = [\n",
    "    \"dbo.Orders\",\n",
    "    \"dbo.UserProfile\",\n",
    "    \"(SELECT TOP 100 * FROM dbo.RandomDataWith100Columns) AS sample\"\n",
    "]\n",
    "\n",
    "batch_results = create_batch_transfer_pattern(tables)\n",
    "print(f\"Batch results: {batch_results}\")\n",
    "\n",
    "print(\"♻️ Patterns can be reused across projects and teams!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Summary: When to Use Mid-Level API\n",
    "\n",
    "**Use this API when:**\n",
    "- ✅ You need custom workflows but want to stay productive\n",
    "- ✅ You want to build reusable patterns for your team\n",
    "- ✅ You need explicit control over connections and batching\n",
    "- ✅ You want to see what's happening in your pipelines\n",
    "- ✅ You're building data engineering tools\n",
    "\n",
    "**Don't use this API when:**\n",
    "- ❌ You just want to copy data quickly (use High-Level API)\n",
    "- ❌ You need to access raw Snowpark operations\n",
    "- ❌ You want to customize every aspect of the transfer\n",
    "- ❌ You're building low-level database tools\n",
    "\n",
    "**What you learned:**\n",
    "- `TransferBuilder` - Fluent interface for building transfers\n",
    "- `ConnectionManager` - Explicit connection control and batching\n",
    "- `Pipeline` - Composable transformation chains\n",
    "- `show_pipeline_steps()` - Transparency into what's happening\n",
    "- Reusable patterns for team productivity\n",
    "\n",
    "**Next:** Check out the Low-Level API for complete control over transfers.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
